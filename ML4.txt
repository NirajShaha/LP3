1 ASSIGNMENT-4
Given a bank customer, build a neural network-based classifier that can determine whether they
will leave or not in the next 6 months. Dataset Description: The case study is from an opensource dataset from Kaggle. The dataset contains 10,000 sample points with 14 distinct features such as CustomerId, CreditScore, Geography, Gender, Age, Tenure, Balance, etc. Dataset:
https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling Perform following steps:
1. Read the dataset. 2. Distinguish the feature and target set and divide the data set into training
and test sets. 3. Normalize the train and test data. 4. Initialize and build the model. Identify the
points of improvement and implement the same. 5. Print the accuracy score and confusion matrix
(5 points).
import numpy as np
import pandas as pd
df = pd.read_csv('churn_Modelling.csv', index_col='RowNumber')
df.head()
df.info()
# Check for null values
df.isnull().values.any()
df.describe()
x_columns = df.columns.tolist()[2:12]
y_columns = df.columns.tolist()[-1:]
print(f'All columns: {df.columns.tolist()}')
print(f'X values: {x_columns}')
print(f'y values: {y_columns}')
x = df[x_columns].values # Credit Score through Estimated Salary
y = df[y_columns].values # Exited
1.1 PREPROCESSING
1.1.1 LABEL ENCODING
from sklearn.preprocessing import LabelEncoder
print(x[:8,1], '... will now become: ')
label_x_country_encoder = LabelEncoder()
x[:,1] = label_x_country_encoder.fit_transform(x[:,1])
print(x[:8,1])
print(x[:6,2], '... will now become: ')
label_x_gender_encoder = LabelEncoder()
x[:,2] = label_x_gender_encoder.fit_transform(x[:,2])
print(x[:6,2])
1.1.2 ONE-HOT ENCODING
The Problem here is that we are treating the countries as one variable with ordinal values (0 < 1 <
2). Therefore, one way to get rid of that problem is to split the countries into respective dimensions.
that is, | Country | -> | Country|-> |Spain|France|Germany|
|——| |——| |——| |——| |——|
| Spain | -> |0| -> |1|0|0|
| France | -> |1| -> |0|1|0|
| Germany | -> |2| -> |0|0|1|
You can now see that the first three columns represent the three countries that constituted the
“country” category. We can now observe that we essentially only need two columns: a 0 on two
countries means that the country has to be the one variable which wasn’t included. This will save us
from the problem of using too many dimensions.
|Spain|France|Germany|-> |France|Germany| |——| |——| |——| |——| |——|
|1|0|0|-> |0|0|
|0|1|0|-> |1|0|
|0|1|0|-> |1|0|
|0|0|1|-> |0|1|
We have achieved this using the drop='first' option in the OneHotEncoder
1.1.3 FEATURE SCALING
Feature scaling is a method used to standardize the range of independent variables or features of
data. It is basically scaling all the dimensions to be even so that one independent variable does not
dominate another. For example, bank account balance ranges from millions to 0, whereas gender is
either 0 or 1. If one of the features has a broad range of values, the distance will be governed by this
particular feature. Therefore, the range of all features should be normalized so that each feature
contributes approximately proportionately to the final distance.
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
pipeline = Pipeline(
[('Categorizer', ColumnTransformer(
[ # Gender
("Gender Label encoder", OneHotEncoder(categories='auto',␣
↪
), ' first '= drop
]), 2[
#
Geography
("Geography One Hot", OneHotEncoder(categories='auto', drop='first'),␣
↪
[1])
], remainder='passthrough', n_jobs=1)),
# Standard Scaler for the classifier
('Normalizer', StandardScaler())
])
x = pipeline.fit_transform(x)
1.2 Making the NN
from keras.models import Sequential
from keras.layers import Dense, Dropout
# Initializing the ANN
classifier = Sequential()
# Splitting the dataset into the Training and Testing set.
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2,␣
↪
random_state
print(f'training shapes: {x_train.shape}, {y_train.shape}')
print(f'testing shapes: {x_test.shape}, {y_test.shape}')
1.2.1 ADDING INPUT LAYER
The breakdown of the inputs for the first layer is as follows: units: 6 nodes (number of
nodes in hidden layer). Can think of this as number of nodes are in the next layer.
activiation: relu becasue we are in an input layer. uses the ReLu activation function for the
layer. This is equivalent to 𝑚𝑎𝑥(0, 𝑊 × 𝑥
𝑇
+ 𝑏)
input_dim: 11 because we span 11 dimensions in our input layer. This is needed for the first added
layer. The subsequent layers’s input dimensions can be inferred using the previously added layer’s
output dimension. The next hidden layer will know what to expect.
classifier.add(Dense(6, activation = 'relu', input_shape = (x_train.shape[1],classifier.add(Dropout(rate=0.1)))))classifier.add(Dropout(rate=0.1))
1.2.2 ADDING 2ND HIDDEN LAYER
We will make our second hidden layer also have 6 nodes, just playing with the same arithmetic
we used to determine the dimensions of the first hidden layer (average of your input and output
layers) $(11+1)÷2 = 6 $.
# Adding the second hidden layer
# Notice that we do not need to specify input dim.
classifier.add(Dense(6, activation = 'relu'))
classifier.add(Dropout(rate=0.1))
1.2.3 Adding the output layer
The breakdown of the inputs for the output layer is as follows: activiation: sigmoid
becasue we are in an output layer. uses the Sigmoid activation function for 𝜙
.
This is used instead
of the ReLu function becasue it generates probabilities for the outcome. We want the probability
that each customer leaves the bank.
units: 6 node(numb of nodes in hidden layer). Can think of this as number of nodes are in the
next layer.
input_dim: 11 because we span 11 dimensions in our input layer. This is needed for the first added
layer. The subsequent layers’s input dimensions can be inferred using the previously added layer’s
output dimension. The next hidden layer will know what to expect.
# Adding the output layer
# Notice that we do not need to specify input dim.
# we have an output of 1 node, which is the the desired dimensions of our␣
↪
output
er
# We use the sigmoid because we want probability outcomes
classifier.add(Dense(1, activation = 'sigmoid'))
classifier.summary()
1.3 Compiling the Neural Network
The breakdown of the inputs for compiling is as follows: optimizer: adam The algorithm
we want to use to find the optimal set of weights in the neural networks. Adam is a very efficeint
variation of Stochastic Gradient Descent.
loss: binary_crossentropy This is the loss function used within adam. This should be the
logarthmic loss. If our dependent (output variable) is Binary, it is binary_crossentropy. If
Categorical, then it is called categorical_crossentropy
metrics: [accuracy] The accuracy metrics which will be evaluated(minimized) by the model.
Used as accuracy criteria to imporve model performance.
classifier.compile(optimizer='adam', loss = 'binary_crossentropy',
↪
metrics=['accuracy'])
history = classifier.fit(x_train, y_train, batch_size=32, epochs=200,
validation_split=0.1, verbose=2)
1.4 Fitting the Neural Network
This is where we will be fitting the NN to our training set.
The breakdown of the inputs for compiling is as follows: X_train The independent variable portion of the data which needs to be fitted with the model.
Y_train The output portion of the data which the model needs to produce after fitting.
batch_size: How often we want to back-propogate the error values so that individual node weights
can be adjusted.
epochs: The number of times we want to run the entire test data over again to tune the weights.
This is like the fuel of the algorithm.
validation_split: 0.2 The fraction of data to use for validation data.
history = classifier.fit(x_train, y_train, batch_size=32, epochs=200,␣
↪
validation_split=0.1,
1.5 TESTING THE NN
y_pred = classifier.predict(x_test)
print(y_pred[:5])
y_pred = (y_pred > 0.5).astype(int)
print(y_pred[:5])
1.6 REPORT
from sklearn.metrics import classification_report, confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix")
print(cm)
cr = classification_report(y_test, y_pred)
print("Classification Report")
print(cr)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
accuracy =(tp+tn)/(tp+tn+fp+fn)
precision =(tp)/(tp+fp)
recall =(tp)/(tp+fn)
f1_score =2*(( precision * recall)/( precision + recall))
print(
'Accuracy:\t',accuracy*100,
'\nPrecision:\t',precision*100,
'\nRecall: \t',recall*100,
'\nF1-Score:\t',f1_score*100)